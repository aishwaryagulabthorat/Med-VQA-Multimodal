# Med-VQA: Multimodal Medical Visual Question Answering

A research project exploring **Medical Visual Question Answering (Med-VQA)** using **multimodal Retrieval-Augmented Generation (RAG)** and advanced prompting strategies for enhanced accuracy and explainability.

## ğŸ§  Overview
This project builds a system to answer medical questions based on input images (e.g., X-rays, MRIs) by integrating both **textual and visual evidence**. It leverages:
- Prompting techniques: Few-Shot, Chain-of-Thought (CoT), Tree-of-Thought (ToT)
- Retrieval-Augmented Generation using PubMedQA, SLAKE, and PMC-VQA
- Lightweight fine-tuning (LoRA) of LLaVA-Med

## ğŸ“Š Results
- **Prompting Recall**: Up to 77% (CoT, ToT)
- **Fine-tuned LLaVA-Med Accuracy**: 54.27%
- **RAG Enhanced Recall**: 71.83%

## ğŸ“ Repository Structure
Med-VQA-Project/
â”œâ”€â”€ code/ # Model training, RAG, retrieval modules
â”œâ”€â”€ reports/ # Final project and proposal PDFs
â”œâ”€â”€ README.md # Project overview and instructions


## ğŸ“š Datasets Used
- **PMC-VQA** â€“ 227k image-question pairs from PubMed Central
- **SLAKE** â€“ Visual dataset with 14k QA pairs
- **PubMedQA** â€“ Biomedical text QA dataset for retrieval context

## ğŸ›  Technologies
- Python, PyTorch, FAISS
- CLIP, MiniLM, GPT-4 API, LLaVA-Med, LoRA

## ğŸš€ Features
- Zero-shot and Few-shot image QA
- Visual + text-based retrieval
- Step-by-step answer justification (explainability)
- LoRA-tuned lightweight vision-language model

## ğŸ“„ License
This project is for academic use only.
